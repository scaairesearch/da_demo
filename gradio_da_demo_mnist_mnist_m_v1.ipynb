{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlnGXRfaXP9oTU7/JTO+Uf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scaairesearch/da_demo/blob/main/gradio_da_demo_mnist_mnist_m_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAfh_kCOTy8a",
        "outputId": "4870a2ac-6c77-4642-b34a-1c039c42ec6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.2/318.2 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import datasets,transforms\n",
        "import random\n",
        "import torch"
      ],
      "metadata": {
        "id": "1IZ8Tw4mUtwZ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_image():\n",
        "    # Load the image from a local file\n",
        "    image = Image.open(\"/content/mnist-m.JPG\")\n",
        "    return image"
      ],
      "metadata": {
        "id": "CM32KxlGXwY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = 28 # for mnist\n",
        "cpu_batch_size = 10\n",
        "# cuda_batch_size = 128\n",
        "# EPOCHS = 1 # 15 if cuda else 2\n",
        "\n",
        "# target_drive_path = '/content/gdrive/MyDrive/mnist_m/dataset/mnist_m.tar.gz'\n",
        "# model_drive_path = '/content/gdrive/MyDrive/da_demo/cv/models'\n",
        "# demo_data_path = '/content/gdrive/MyDrive/da_demo/data'\n",
        "\n",
        "# data_base_path = '/content/data'\n",
        "# source_dataset_name = 'MNIST'\n",
        "# target_dataset_name = 'mnist_m'\n",
        "# source_image_root = os.path.join(data_base_path, source_dataset_name)\n",
        "# target_image_root = os.path.join(data_base_path, target_dataset_name)\n",
        "# model_root = 'models'\n",
        "# cuda = True\n",
        "# cudnn.benchmark = True\n",
        "# lr = 1e-3\n",
        "# batch_size = 128\n",
        "# image_size = 28\n",
        "# n_epoch = 100"
      ],
      "metadata": {
        "id": "LKi6p6HdlXvP"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SOURCE\n"
      ],
      "metadata": {
        "id": "aw5VQmBRVPJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                       transforms.Resize(img_size),\n",
        "                                       transforms.ToTensor(),# converts to tesnor\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n",
        "\n",
        "display_transforms = transforms.Compose([\n",
        "                                       transforms.Resize(img_size),\n",
        "                                      #  transforms.ToTensor(),# converts to tesnor\n",
        "                                      #  transforms.Normalize((0.1307,), (0.3081,)),\n",
        "                                       transforms.ToPILImage()\n",
        "                                       ])\n",
        "\n",
        "test = datasets.MNIST('./data',\n",
        "                      train=False,\n",
        "                      download=True,\n",
        "                      transform=test_transforms)\n",
        "\n",
        "dataloader_args = dict(shuffle=True, batch_size=cpu_batch_size)\n",
        "\n",
        "mnist_loader = torch.utils.data.DataLoader(\n",
        "    dataset = test,\n",
        "    **dataloader_args\n",
        ")"
      ],
      "metadata": {
        "id": "kWffYSr9lFS-"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "    print(\"Device Selected:\", device)\n",
        "    return device\n",
        "\n",
        "device = get_device()\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJPeTHVBzN8n",
        "outputId": "fb9fbb26-0ac0-4882-8ab2-773dff229560"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device Selected: cpu\n",
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mnist_images(num=10):\n",
        "  images, labels = next(iter(mnist_loader))\n",
        "  for image, label in zip (images,labels):\n",
        ""
      ],
      "metadata": {
        "id": "7uvigTBynUiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_image(image, num_classes=3):\n",
        "  # transforming image and getting prediction from model\n",
        "  transformed_image = test_transforms(image)\n",
        "  image_tensor = transformed_image.to(device).unsqueeze(0)#transform(torch.tensor(image).to(DEVICE)).unsqueeze(0) # making it a batch\n",
        "\n",
        "  # sending it to model to get prediction\n",
        "  logits = loaded_model(image_tensor) # logits\n",
        "  output = F.softmax(logits.view(-1)) #F.softmax(output.flatten(), dim=-1) #\n",
        "\n",
        "  confidences = [(classes[i], float(output[i])) for i in range(len(classes))]\n",
        "  confidences.sort(key=lambda x: x[1], reverse=True)\n",
        "  confidences = OrderedDict(confidences[:num_classes])\n",
        "  label = torch.argmax(output).item()"
      ],
      "metadata": {
        "id": "lG0iAFikyx_a"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "  with gr.Tab(\"MNIST-M : inference NonDANN and DANN\"):\n",
        "    with gr.Row():\n",
        "      radio_gradcam = gr.Radio([\"Baseline (Non-DANN)\", \"DANN\"],\n",
        "                               label=\"Select the model you want to use.\",\n",
        "                               value=\"Baseline (Non-DANN)\",  # Set default value\n",
        "                               scale=2)\n",
        "\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        input_image_classify = gr.Image(label=\"Classify Digit\", type = \"pil\", height = 256, width = 256)\n",
        "        button_classify = gr.Button(\"Submit to Classify Image\", visible = True)\n",
        "\n",
        "\n",
        "\n",
        "      with gr.Column():\n",
        "        label_classify = gr.Label(num_top_classes=10, visible = True)\n",
        "\n",
        "    with gr.Row():\n",
        "      # gr.Markdown(\"Select the Image\")\n",
        "      for batch in mnist_loader:\n",
        "        images, labels = batch[0],batch[1]\n",
        "        image_pils = [display_transforms(img) for img in images]\n",
        "        # print(labels)\n",
        "        # for img,label in zip (images, labels):\n",
        "        #   img_pil = display_transforms(img)\n",
        "        #   # with gr.Column():\n",
        "        #   gr.Examples([img_pil],inputs=[input_image_classify],label=label.item())\n",
        "        # break\n",
        "        gr.Examples(image_pils,inputs=[input_image_classify], label = \"Select an example MNIST Image\")\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ##########################\n",
        "  with gr.Tab(\"Introduction\"):\n",
        "    gr.Markdown(\"## Domain Adaptation in Deep Networks - Demonstration\")\n",
        "    gr.Markdown(\n",
        "        '''\n",
        "        Source - MNIST\n",
        "        ------\n",
        "        - The MNIST database (Modified National Institute of Standards and Technology database) is a large collection of handwritten digits.\n",
        "        - It has a training set of 60,000 examples, and a test set of 10,000 examples.\n",
        "        - 28 x 28 size\n",
        "        - 1 channel\n",
        "\n",
        "        '''\n",
        "        )\n",
        "    gr.Markdown(\n",
        "        '''\n",
        "        Target - MNIST-M\n",
        "        -------\n",
        "        - MNIST-M is created by combining MNIST digits with the patches randomly extracted from color photos of BSDS500 as their background.\n",
        "        - It contains 59,001 training and 90,001 test images.\n",
        "        - 28 x 28 size\n",
        "        - 3 channels\n",
        "        '''\n",
        "    )\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "             image_output = gr.Image(value=display_image(), label = \"source and target\",height = 512, width = 512, show_label = True)\n",
        "  ################\n",
        "\n",
        "  button_classify.click(fn=classify_image, inputs =[input_image_classify,slider_classify_num_classes,checkbox_gradcam_classify,dropdown_gradcam_classify_layer,slider_gradcam_classify_opacity], outputs = [label_classify,gallery_gradcam_classify,gallery_gradcam_classify])\n",
        "\n",
        "\n",
        "  # radio_gradcam.change(fn=view_gradcam_images, inputs=radio_gradcam, outputs=[slider_gradcam_num_images, dropdown_gradcam_layer,slider_gradcam_opacity,button_gradcam, output_gallery_gradcam])\n",
        "\n",
        "\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "iKiD5b3yU5pf",
        "outputId": "0fb7ca36-29b0-4229-ac38-a2db2e2a73ba"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://444ece5ec8ab96a3ab.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://444ece5ec8ab96a3ab.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import gradio as gr\n",
        "\n",
        "# def display_image(image):\n",
        "#     return image\n",
        "\n",
        "# iface = gr.Interface(\n",
        "#     fn=display_image,\n",
        "#     inputs=gr.inputs.Image(type=\"pil\"),\n",
        "#     outputs=gr.outputs.Image(type=\"pil\"),\n",
        "#     title=\"Image Display\",\n",
        "#     description=\"Upload an image to display it.\"\n",
        "# )\n",
        "\n",
        "# iface.launch()\n"
      ],
      "metadata": {
        "id": "TUSOxIIDUNBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the Gradio app\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "3ZVJv7mYUr5z",
        "outputId": "84edc790-3838-460e-8314-d36c87f50aa1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://0d6269534cc6fe1ba0.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0d6269534cc6fe1ba0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "\n",
        "# Dummy dataset and dataloader for demonstration\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, transform=None):\n",
        "        self.transform = transform\n",
        "        # Assuming the dataset consists of images and labels\n",
        "        self.data = [\n",
        "            ('/content/mnist-m.JPG', 0)\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        image = Image.open(img_path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label, img_path\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Initialize dataset and dataloader\n",
        "dataset = CustomDataset(transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "# Function to get 10 random images\n",
        "def get_random_images():\n",
        "    data_iter = iter(dataloader)\n",
        "    images, labels, paths = next(data_iter)\n",
        "    images = [transforms.ToPILImage()(img) for img in images]\n",
        "    return images, paths\n",
        "\n",
        "# Dummy model for demonstration\n",
        "def classify_digit(image):\n",
        "    # Replace this with actual model inference code\n",
        "    return \"Predicted Label\"\n",
        "\n",
        "# Initialize 10 random images\n",
        "images, paths = get_random_images()\n",
        "\n",
        "# Create Gradio interface\n",
        "def display_images(image_path):\n",
        "    # Find the corresponding image from paths\n",
        "    selected_image = None\n",
        "    for img, path in zip(images, paths):\n",
        "        if path == image_path:\n",
        "            selected_image = img\n",
        "            break\n",
        "\n",
        "    if selected_image:\n",
        "        return selected_image, classify_digit(selected_image)\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Select an Image and Classify Digit\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # Display 10 random images\n",
        "        for img, path in zip(images, paths):\n",
        "            with gr.Column():\n",
        "                gr.Image(value=img, label=path, interactive=True).click(\n",
        "                    display_images, inputs=path, outputs=[\"image_output\", \"label_classify\"]\n",
        "                )\n",
        "\n",
        "    with gr.Row():\n",
        "        image_output = gr.Image(label=\"Selected Image\")\n",
        "        label_classify = gr.Label(label=\"Classification Result\")\n",
        "\n",
        "# Launch the Gradio app\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "SbR-Ksm9jrvw",
        "outputId": "a42ed9e0-3eb4-4822-a170-732a553e36cf"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Image' object has no attribute 'click'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-54c54fc1a6eb>\u001b[0m in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 gr.Image(value=img, label=path, interactive=True).click(\n\u001b[0m\u001b[1;32m     72\u001b[0m                     \u001b[0mdisplay_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_output\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label_classify\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Image' object has no attribute 'click'"
          ]
        }
      ]
    }
  ]
}