{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scaairesearch/da_demo/blob/main/gradio_da_demo_mnist_mnist_m_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAfh_kCOTy8a"
      },
      "outputs": [],
      "source": [
        "!pip install gradio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1IZ8Tw4mUtwZ"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import datasets,transforms\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Function\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "import io\n",
        "import base64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CM32KxlGXwY_"
      },
      "outputs": [],
      "source": [
        "def display_image():\n",
        "    # Load the image from a local file\n",
        "    image = Image.open(\"/content/mnist-m.JPG\")\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LKi6p6HdlXvP"
      },
      "outputs": [],
      "source": [
        "img_size = 28 # for mnist\n",
        "cpu_batch_size = 10\n",
        "class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "\n",
        "# target_dataset_name = 'mnist_m'\n",
        "# target_image_root = os.path.join(data_base_path, target_dataset_name)\n",
        "# cuda_batch_size = 128\n",
        "# EPOCHS = 1 # 15 if cuda else 2\n",
        "\n",
        "# target_drive_path = '/content/gdrive/MyDrive/mnist_m/dataset/mnist_m.tar.gz'\n",
        "# model_drive_path = '/content/gdrive/MyDrive/da_demo/cv/models'\n",
        "# demo_data_path = '/content/gdrive/MyDrive/da_demo/data'\n",
        "\n",
        "# data_base_path = '/content/data'\n",
        "# source_dataset_name = 'MNIST'\n",
        "# target_dataset_name = 'mnist_m'\n",
        "# source_image_root = os.path.join(data_base_path, source_dataset_name)\n",
        "# target_image_root = os.path.join(data_base_path, target_dataset_name)\n",
        "# model_root = 'models'\n",
        "# cuda = True\n",
        "# cudnn.benchmark = True\n",
        "# lr = 1e-3\n",
        "# batch_size = 128\n",
        "# image_size = 28\n",
        "# n_epoch = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw5VQmBRVPJA"
      },
      "source": [
        "### SOURCE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kWffYSr9lFS-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "111b0471-9096-4f7d-a368-15ca2c888b0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 51166894.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1676154.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 12468321.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 7823625.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize(img_size),\n",
        "                                       transforms.ToTensor(),# converts to tesnor\n",
        "                                      #  transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n",
        "\n",
        "# display_transforms = transforms.Compose([\n",
        "#                                        transforms.Resize(img_size),\n",
        "#                                       #  transforms.ToTensor(),# converts to tesnor\n",
        "#                                       #  transforms.Normalize((0.1307,), (0.3081,)),\n",
        "#                                        transforms.ToPILImage()\n",
        "#                                        ])\n",
        "\n",
        "transform_to_pil  = transforms.ToPILImage()\n",
        "\n",
        "test = datasets.MNIST('./data',\n",
        "                      train=False,\n",
        "                      download=True,\n",
        "                      transform=test_transforms)\n",
        "\n",
        "dataloader_args = dict(shuffle=True, batch_size=cpu_batch_size)\n",
        "\n",
        "mnist_loader = torch.utils.data.DataLoader(\n",
        "    dataset = test,\n",
        "    **dataloader_args\n",
        ")\n",
        "\n",
        "def get_mnist_images():\n",
        "    images, labels = next(iter(mnist_loader))\n",
        "    pil_images = [transform_to_pil(image) for image in images]\n",
        "    return pil_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IENxJOsf4FB7"
      },
      "source": [
        "## TARGET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Tzg3OMo44xOo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07e9fd97-30aa-4910-ec0d-55acabe04630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "splits = {'train': 'data/train-00000-of-00001-571b6b1e2c195186.parquet', 'test': 'data/test-00000-of-00001-ba3ad971b105ff65.parquet'}\n",
        "df = pd.read_parquet(\"hf://datasets/Mike0307/MNIST-M/\" + splits[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "U5nxgGCL6Uac"
      },
      "outputs": [],
      "source": [
        "# df['image'].iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9mcScVxh41T_"
      },
      "outputs": [],
      "source": [
        "class MNIST_M(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "      # splits = {'train': 'data/train-00000-of-00001-571b6b1e2c195186.parquet', 'test': 'data/test-00000-of-00001-ba3ad971b105ff65.parquet'}\n",
        "      # self.dataframe = pd.read_parquet(\"hf://datasets/Mike0307/MNIST-M/\" + splits[\"test\"])\n",
        "      self.dataframe = dataframe\n",
        "      self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image and label from dataframe\n",
        "        img_data = self.dataframe.iloc[idx]['image']['bytes']\n",
        "        label = self.dataframe.iloc[idx]['label']\n",
        "        img_path = self.dataframe.iloc[idx]['image']['path']\n",
        "\n",
        "        # Decode image data (assuming it's base64 encoded)\n",
        "        img = Image.open(io.BytesIO(img_data))\n",
        "\n",
        "        # img = Image.open(io.BytesIO(base64.b64decode(img_data)))\n",
        "\n",
        "        # Apply transformations if any\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label,img_path\n",
        "\n",
        "\n",
        "# Test Phase transformations\n",
        "target_test_transforms = transforms.Compose([\n",
        "                                       transforms.Resize(img_size),\n",
        "                                       transforms.ToTensor(),# converts to tesnor\n",
        "                                       transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "                                       ])\n",
        "\n",
        "# transform_to_pil  = transforms.Compose([\n",
        "#                                        transforms.Resize(img_size),\n",
        "#                                        transforms.ToTensor(),# converts to tesnor\n",
        "#                                        transforms.ToPILImage()\n",
        "#                                        ])\n",
        "\n",
        "transform_to_pil  = transforms.ToPILImage()\n",
        "\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "target_test_dataset = MNIST_M(dataframe=df, transform=target_test_transforms)\n",
        "target_test_dataloader = torch.utils.data.DataLoader(target_test_dataset, batch_size=cpu_batch_size, shuffle=True)\n",
        "def get_mnist_m_images():\n",
        "    images, labels,image_names = next(iter(target_test_dataloader))\n",
        "    pil_images = [transform_to_pil(image) for image in images]\n",
        "    return pil_images, labels.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Models"
      ],
      "metadata": {
        "id": "AH29N1HFVa00"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJPeTHVBzN8n",
        "outputId": "9dd4740f-5b95-466a-8210-f897c3a58d68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device Selected: cpu\n",
            "cpu\n"
          ]
        }
      ],
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "    print(\"Device Selected:\", device)\n",
        "    return device\n",
        "\n",
        "device = get_device()\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Gpu0gYyVl1FW"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GradientReversalFn(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        return output, None\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, num_classes = 10):\n",
        "        super(Network, self).__init__()  # Initialize the parent class\n",
        "\n",
        "        drop_out_value = 0.1\n",
        "\n",
        "        #---------------------Feature Extractor Network------------------------#\n",
        "        self.feature_extractor  = nn.Sequential(\n",
        "            # Input Block\n",
        "            nn.Conv2d(3, 16, 3, bias=False),  # In: 3x28x28, Out: 16x26x26, RF: 3x3, Stride: 1\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(drop_out_value),\n",
        "\n",
        "            # Conv Block 2\n",
        "            nn.Conv2d(16, 16, 3, bias=False),  # In: 16x26x26, Out: 16x24x24, RF: 5x5, Stride: 1\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(drop_out_value),\n",
        "\n",
        "            # Conv Block 3\n",
        "            nn.Conv2d(16, 16, 3, bias=False),  # In: 16x24x24, Out: 16x22x22, RF: 7x7, Stride: 1\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(drop_out_value),\n",
        "\n",
        "            # Transition Block 1\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # In: 16x22x22, Out: 16x11x11, RF: 8x8, Stride: 2\n",
        "\n",
        "            # Conv Block 4\n",
        "            nn.Conv2d(16, 16, 3, bias=False),  # In: 16x11x11, Out: 16x9x9, RF: 12x12, Stride: 1\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(drop_out_value),\n",
        "\n",
        "            # Conv Block 5\n",
        "            nn.Conv2d(16, 32, 3, bias=False),  # In: 16x9x9, Out: 32x7x7, RF: 16x16, Stride: 1\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(drop_out_value),\n",
        "\n",
        "            # Output Block\n",
        "            nn.Conv2d(32, 64, 1, bias=False),  # In: 32x7x7, Out: 64x7x7, RF: 16x16, Stride: 1\n",
        "\n",
        "            # Global Average Pooling\n",
        "            nn.AvgPool2d(7)  # In: 64x7x7, Out: 64x1x1, RF: 16x16, Stride: 7\n",
        "        )\n",
        "\n",
        "        #---------------------Class Classifier Network------------------------#\n",
        "        self.class_classifier = nn.Sequential(nn.ReLU(),\n",
        "                                        nn.Dropout(p=drop_out_value),\n",
        "                                        nn.Linear(64,50),\n",
        "                                        nn.BatchNorm1d(50), # added batch norm to improve accuracy\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Dropout(p=drop_out_value),\n",
        "                                        nn.Linear(50,num_classes))\n",
        "\n",
        "        #---------------------Label Classifier Network------------------------#\n",
        "        self.domain_classifier = nn.Sequential(nn.ReLU(),\n",
        "                                        nn.Dropout(p=drop_out_value),\n",
        "                                        nn.Linear(64,50),\n",
        "                                        nn.BatchNorm1d(50), # added batch norm to improve accuracy\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Dropout(p=drop_out_value),\n",
        "                                        nn.Linear(50,2))\n",
        "    def forward(self, input_data, alpha = 1.0):\n",
        "      if input_data.data.shape[1] == 1:\n",
        "        input_data = input_data.expand(input_data.data.shape[0], 3, img_size, img_size)\n",
        "\n",
        "      input_data = self.feature_extractor(input_data)\n",
        "\n",
        "      features = input_data.view(input_data.size(0), -1)  # Flatten the output for fully connected layer\n",
        "\n",
        "      reverse_features = GradientReversalFn.apply(features, alpha)\n",
        "      class_output = self.class_classifier(features)\n",
        "      domain_output = self.domain_classifier(reverse_features)\n",
        "\n",
        "      return class_output, domain_output, features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nQ20BxHvmCPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7636e95-1b67-43da-f787-1058812b19e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (feature_extractor): Sequential(\n",
              "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Dropout(p=0.1, inplace=False)\n",
              "    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (5): ReLU()\n",
              "    (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): Dropout(p=0.1, inplace=False)\n",
              "    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (9): ReLU()\n",
              "    (10): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): Dropout(p=0.1, inplace=False)\n",
              "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (13): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (14): ReLU()\n",
              "    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (16): Dropout(p=0.1, inplace=False)\n",
              "    (17): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (18): ReLU()\n",
              "    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (20): Dropout(p=0.1, inplace=False)\n",
              "    (21): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (22): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
              "  )\n",
              "  (class_classifier): Sequential(\n",
              "    (0): ReLU()\n",
              "    (1): Dropout(p=0.1, inplace=False)\n",
              "    (2): Linear(in_features=64, out_features=50, bias=True)\n",
              "    (3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.1, inplace=False)\n",
              "    (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "  )\n",
              "  (domain_classifier): Sequential(\n",
              "    (0): ReLU()\n",
              "    (1): Dropout(p=0.1, inplace=False)\n",
              "    (2): Linear(in_features=64, out_features=50, bias=True)\n",
              "    (3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.1, inplace=False)\n",
              "    (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "## NON DANN\n",
        "# Instantiate the model (make sure it has the same architecture)\n",
        "loaded_model_non_dann = Network()\n",
        "loaded_model_non_dann = loaded_model_non_dann.to(device)\n",
        "# Load the saved state dictionary\n",
        "loaded_model_non_dann.load_state_dict(torch.load('/content/non_dann_26_06.pt', map_location=device), strict=False)\n",
        "loaded_model_non_dann.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FYg3SXbzmrp4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe05a56-1433-4d79-faca-ccc678f101ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (feature_extractor): Sequential(\n",
              "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Dropout(p=0.1, inplace=False)\n",
              "    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (5): ReLU()\n",
              "    (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): Dropout(p=0.1, inplace=False)\n",
              "    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (9): ReLU()\n",
              "    (10): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): Dropout(p=0.1, inplace=False)\n",
              "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (13): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (14): ReLU()\n",
              "    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (16): Dropout(p=0.1, inplace=False)\n",
              "    (17): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (18): ReLU()\n",
              "    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (20): Dropout(p=0.1, inplace=False)\n",
              "    (21): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (22): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
              "  )\n",
              "  (class_classifier): Sequential(\n",
              "    (0): ReLU()\n",
              "    (1): Dropout(p=0.1, inplace=False)\n",
              "    (2): Linear(in_features=64, out_features=50, bias=True)\n",
              "    (3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.1, inplace=False)\n",
              "    (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "  )\n",
              "  (domain_classifier): Sequential(\n",
              "    (0): ReLU()\n",
              "    (1): Dropout(p=0.1, inplace=False)\n",
              "    (2): Linear(in_features=64, out_features=50, bias=True)\n",
              "    (3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.1, inplace=False)\n",
              "    (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "##  DANN\n",
        "# Instantiate the model (make sure it has the same architecture)\n",
        "loaded_model_dann = Network()\n",
        "loaded_model_dann = loaded_model_dann.to(device)\n",
        "# Load the saved state dictionary\n",
        "loaded_model_dann.load_state_dict(torch.load('/content/dann_26_06.pt', map_location=device), strict=False)\n",
        "loaded_model_dann.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classify Image"
      ],
      "metadata": {
        "id": "IOdmPv3Iuxp1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "5VK39vp9nAei"
      },
      "outputs": [],
      "source": [
        "def classify_image(image,\n",
        "                   select_model\n",
        "                  #  ,mnist_images, mnist_m_images\n",
        "                   ):\n",
        "  source = \"Unknown\"\n",
        "  if image in mnist_images:\n",
        "      source = \"MNIST\"\n",
        "  elif image in mnist_m_images:\n",
        "      source = \"MNIST-M\"\n",
        "\n",
        "\n",
        "\n",
        "  if select_model == \"Baseline (Non-DANN)\":\n",
        "    model = loaded_model_non_dann\n",
        "    test_transforms = transforms.Compose([\n",
        "                                       transforms.Resize(img_size),\n",
        "                                       transforms.ToTensor(),# converts to tesnor\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n",
        "    transformed_image = test_transforms(image)\n",
        "    image_tensor = transformed_image.to(device).unsqueeze(0)\n",
        "  if select_model == \"DANN\":\n",
        "    ## TO DO Transforms for DANN\n",
        "    model = loaded_model_dann\n",
        "    target_test_transforms = transforms.Compose([\n",
        "                                       transforms.Resize(img_size),\n",
        "                                       transforms.ToTensor(),# converts to tesnor\n",
        "                                       transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "                                       ])\n",
        "    target_transformed_image = target_test_transforms(image)\n",
        "    image_tensor = target_transformed_image.to(device).unsqueeze(0)\n",
        "\n",
        "\n",
        "  logits,_,_ = model(image_tensor)\n",
        "  output = F.softmax(logits.view(-1), dim = -1) #F.softmax(output.flatten(), dim=-1) #\n",
        "\n",
        "  confidences = [(class_names[i], float(output[i])) for i in range(len(class_names))]\n",
        "  confidences.sort(key=lambda x: x[1], reverse=True)\n",
        "  confidences = OrderedDict(confidences[:3])\n",
        "  label = torch.argmax(output).item()\n",
        "\n",
        "\n",
        "  # output =  torch.argmax(F.softmax(class_output.view(-1))).item()\n",
        "  # label_output = class_output.argmax(dim=1)#.cpu().numpy()\n",
        "  # label_output = torch.argmax(label_output).item()\n",
        "\n",
        "  return confidences#, source\n",
        "  # return f'{select_model} {label_output} {output}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NXBeFmaCCzo0"
      },
      "outputs": [],
      "source": [
        "# Function to set the source of the selected image\n",
        "def set_image_source(image, source, state):\n",
        "    state['source'] = source\n",
        "    return state['source']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the interaction for updating ground truth label\n",
        "# def update_ground_truth(image):\n",
        "#     idx = mnist_images.index(image)\n",
        "#     return mnist_labels[idx]"
      ],
      "metadata": {
        "id": "ax2KkCev8f2x"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 4 - Case 1\n",
        "\n",
        "MNIST M images not classified correctly with NON DANN but classified correctly with DANN"
      ],
      "metadata": {
        "id": "bs7ZZmznHrJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "case1_list= torch.load('/content/list_mnist_m_non_dann_misclassified_dann_classified.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "DmczsdBUJt6t",
        "outputId": "ab42fed4-e105-4b1d-e2e2-606e7c0c8e4e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-adf9e9575942>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcase1_list\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/list_mnist_m_non_dann_misclassified_dann_classified.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0morig_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0moverall_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m                     warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "  with gr.Tab(\"Case 1: MNIST_M_Non_DANN_Misclassify_DANN_Classify\"):\n",
        "    gr.Markdown(value = \"XXX\")\n",
        "\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "zshSYGF0HnX1",
        "outputId": "dd656823-9327-45f6-956d-da51e780970a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://0f1aae1f7e2954721a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0f1aae1f7e2954721a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://0f1aae1f7e2954721a.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    mnist_images = get_mnist_images()\n",
        "    mnist_m_images,mnist_m_labels = get_mnist_m_images()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "HTNAdGZCrikn",
        "outputId": "ac440d0d-481c-47f7-ef1b-417c9e5534b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'get_mnist_images' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0604f0ecec95>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmnist_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mnist_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmnist_m_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmnist_m_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mnist_m_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_mnist_images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 3 - Working - Inference and Introduction"
      ],
      "metadata": {
        "id": "_-lt2q1T68il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### NEW VERSION V3\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "  with gr.Tab(\"MNIST-M : inference NonDANN and DANN\"):\n",
        "    with gr.Row():\n",
        "      radio_model = gr.Radio([\"Baseline (Non-DANN)\", \"DANN\"],\n",
        "                               label=\"Select the model you want to use.\",\n",
        "                               value=\"Baseline (Non-DANN)\",  # Set default value\n",
        "                               scale=2)\n",
        "    # with gr.Row():\n",
        "    #   radio_dataset = gr.Radio([\"MNIST\", \"MNIST-M\"],\n",
        "    #                            label=\"Select the dataset you want to use.\",\n",
        "    #                            value=\"MNIST\",  # Set default value\n",
        "    #                            scale=2)\n",
        "\n",
        "\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        input_image_classify = gr.Image(label=\"Classify Digit\", type = \"pil\", height = 256, width = 256)\n",
        "        button_classify = gr.Button(\"Submit to Classify Image\", visible = True, size ='sm')\n",
        "\n",
        "\n",
        "      with gr.Column():\n",
        "        with gr.Row():\n",
        "          label_classify = gr.Label(label = \"Predicted label\", num_top_classes=10, visible = True)\n",
        "\n",
        "        # with gr.Row():\n",
        "        #   label_gt = gr.Label(label = \"Ground Truth label\", visible = True)\n",
        "\n",
        "    # # State to store the source of the selected image\n",
        "    # state = gr.State({\"source\": None})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    with gr.Row():\n",
        "      gr.Examples(mnist_images,inputs=[input_image_classify], label = \"Select an example MNIST Image\")\n",
        "\n",
        "\n",
        "    with gr.Row():\n",
        "      gr.Examples(mnist_m_images,inputs=[input_image_classify], label = \"Select an example MNIST-M Image\") #working\n",
        "\n",
        "    # with gr.Row():\n",
        "    #   examples = gr.Examples(\n",
        "    #       examples=[[img] for img in mnist_m_images],\n",
        "    #       inputs=[input_image_classify],\n",
        "    #       # examples=[[img,label] for img,label in zip(mnist_m_images,mnist_m_labels)],\n",
        "    #       # inputs=[input_image_classify,label_gt],\n",
        "    #       label=\"Select an example MNIST Image\")\n",
        "    with gr.Row():\n",
        "      gr.Markdown(value = f'MNIST- M Ground Truth Label = {[label for label in mnist_m_labels]}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ##########################\n",
        "  with gr.Tab(\"Introduction\"):\n",
        "    gr.Markdown(\"## Domain Adaptation in Deep Networks - Demonstration\")\n",
        "    gr.Markdown(\n",
        "        '''\n",
        "        Source - MNIST\n",
        "        ------\n",
        "        - The MNIST database (Modified National Institute of Standards and Technology database) is a large collection of handwritten digits.\n",
        "        - It has a training set of 60,000 examples, and a test set of 10,000 examples.\n",
        "        - 28 x 28 size\n",
        "        - 1 channel\n",
        "\n",
        "        '''\n",
        "        )\n",
        "    gr.Markdown(\n",
        "        '''\n",
        "        Target - MNIST-M\n",
        "        -------\n",
        "        - MNIST-M is created by combining MNIST digits with the patches randomly extracted from color photos of BSDS500 as their background.\n",
        "        - It contains 59,001 training and 90,001 test images.\n",
        "        - 28 x 28 size\n",
        "        - 3 channels\n",
        "        '''\n",
        "    )\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "             image_output = gr.Image(value=display_image(), label = \"source and target\",height = 512, width = 512, show_label = True)\n",
        "  ################\n",
        "\n",
        "  button_classify.click(fn=classify_image,\n",
        "                        inputs=[input_image_classify,radio_model],\n",
        "                        outputs=[label_classify])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "y0J60t2u67hQ",
        "outputId": "a6a552ef-12e0-459b-f9c3-893393ef57db"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://f6659e275a9a19ae0b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f6659e275a9a19ae0b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://f6659e275a9a19ae0b.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "458Ggrcd9HeK"
      },
      "source": [
        "# Version 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "pP3PKRI5cL-Q",
        "outputId": "9c8f79df-77af-45ee-9ba8-00594add61a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:1001: UserWarning: Expected 1 arguments for function <function classify_image at 0x7e9c69984310>, received 4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:1009: UserWarning: Expected maximum 1 arguments for function <function classify_image at 0x7e9c69984310>, received 4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://a9d8c9612ce51430db.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a9d8c9612ce51430db.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://a9d8c9612ce51430db.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "\n",
        "### NEW VERSION V2\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "  with gr.Tab(\"MNIST-M : inference NonDANN and DANN\"):\n",
        "    with gr.Row():\n",
        "      radio_model = gr.Radio([\"Baseline (Non-DANN)\", \"DANN\"],\n",
        "                               label=\"Select the model you want to use.\",\n",
        "                               value=\"Baseline (Non-DANN)\",  # Set default value\n",
        "                               scale=2)\n",
        "    with gr.Row():\n",
        "      radio_dataset = gr.Radio([\"MNIST\", \"MNIST-M\"],\n",
        "                               label=\"Select the dataset you want to use.\",\n",
        "                               value=\"MNIST\",  # Set default value\n",
        "                               scale=2)\n",
        "\n",
        "\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        input_image_classify = gr.Image(label=\"Classify Digit\", type = \"pil\", height = 256, width = 256)\n",
        "        button_classify = gr.Button(\"Submit to Classify Image\", visible = True, size ='sm')\n",
        "\n",
        "\n",
        "      with gr.Column():\n",
        "        with gr.Row():\n",
        "          label_classify = gr.Label(label = \"Predicted label\", num_top_classes=10, visible = True)\n",
        "\n",
        "        with gr.Row():\n",
        "          label_classify_gt = gr.Label(label = \"Ground Truth label\", visible = True)\n",
        "\n",
        "    # State to store the source of the selected image\n",
        "    state = gr.State({\"source\": None})\n",
        "\n",
        "    # with gr.Row():\n",
        "    #     mnist_examples = gr.Examples(get_mnist_images(), inputs=[input_image_classify], label=\"Select an example MNIST Image\")\n",
        "    #     mnist_examples.change(fn=set_image_source, inputs=[input_image_classify, gr.State(value=\"MNIST\"), state], outputs=state)\n",
        "\n",
        "    # with gr.Row():\n",
        "    #     mnist_m_examples = gr.Examples(get_mnist_m_images(), inputs=[input_image_classify], label=\"Select an example MNIST-M Image\")\n",
        "    #     mnist_m_examples.change(fn=set_image_source, inputs=[input_image_classify, gr.State(value=\"MNIST-M\"), state], outputs=state)\n",
        "\n",
        "    mnist_images = get_mnist_images()\n",
        "    mnist_m_images = get_mnist_m_images()\n",
        "    with gr.Row():\n",
        "      # gr.Examples(get_mnist_images(),inputs=[input_image_classify], label = \"Select an example MNIST Image\")\n",
        "      gr.Examples(mnist_images,inputs=[input_image_classify], label = \"Select an example MNIST Image\")\n",
        "\n",
        "\n",
        "    with gr.Row():\n",
        "      # gr.Examples(get_mnist_m_images(),inputs=[input_image_classify], label = \"Select an example MNIST-M Image\")\n",
        "      gr.Examples(mnist_m_images,inputs=[input_image_classify], label = \"Select an example MNIST-M Image\")\n",
        "\n",
        "    # Label to display the source of the selected image\n",
        "    image_source_label = gr.Label(label=\"Selected Image Source\")\n",
        "\n",
        "    # def update_state(image):\n",
        "    #     if image in get_mnist_images():\n",
        "    #         return \"MNIST\"\n",
        "    #     elif image in get_mnist_m_images():\n",
        "    #         return \"MNIST-M\"\n",
        "    #     else:\n",
        "    #         return \"Unknown\"\n",
        "\n",
        "    # input_image_classify.change(fn=update_state, inputs=input_image_classify, outputs=image_source_label)\n",
        "  ##########################\n",
        "  with gr.Tab(\"Introduction\"):\n",
        "    gr.Markdown(\"## Domain Adaptation in Deep Networks - Demonstration\")\n",
        "    gr.Markdown(\n",
        "        '''\n",
        "        Source - MNIST\n",
        "        ------\n",
        "        - The MNIST database (Modified National Institute of Standards and Technology database) is a large collection of handwritten digits.\n",
        "        - It has a training set of 60,000 examples, and a test set of 10,000 examples.\n",
        "        - 28 x 28 size\n",
        "        - 1 channel\n",
        "\n",
        "        '''\n",
        "        )\n",
        "    gr.Markdown(\n",
        "        '''\n",
        "        Target - MNIST-M\n",
        "        -------\n",
        "        - MNIST-M is created by combining MNIST digits with the patches randomly extracted from color photos of BSDS500 as their background.\n",
        "        - It contains 59,001 training and 90,001 test images.\n",
        "        - 28 x 28 size\n",
        "        - 3 channels\n",
        "        '''\n",
        "    )\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "             image_output = gr.Image(value=display_image(), label = \"source and target\",height = 512, width = 512, show_label = True)\n",
        "  ################\n",
        "\n",
        "  # button_classify.click(fn=classify_image, inputs =[input_image_classify,slider_classify_num_classes,checkbox_gradcam_classify,dropdown_gradcam_classify_layer,slider_gradcam_classify_opacity], outputs = [label_classify,gallery_gradcam_classify,gallery_gradcam_classify])\n",
        "  # button_classify.click(fn=classify_image, inputs=[input_image_classify,radio_model,radio_dataset], outputs=label_classify) ## working\n",
        "  button_classify.click(fn=classify_image,\n",
        "                        inputs=[input_image_classify,radio_model,gr.State(mnist_images), gr.State(mnist_m_images)],\n",
        "                        outputs=[label_classify,image_source_label])\n",
        "\n",
        "  # radio_gradcam.change(fn=view_gradcam_images, inputs=radio_gradcam, outputs=[slider_gradcam_num_images, dropdown_gradcam_layer,slider_gradcam_opacity,button_gradcam, output_gallery_gradcam])\n",
        "\n",
        "\n",
        "\n",
        "demo.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGUQw6CRcI4p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "iKiD5b3yU5pf",
        "outputId": "cd0c0d12-4467-4f2a-89d9-7f193ac2bebc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://3c10fcb03a7ecf6937.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://3c10fcb03a7ecf6937.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# ### OLD VERSION V1\n",
        "# with gr.Blocks() as demo:\n",
        "\n",
        "#   with gr.Tab(\"MNIST-M : inference NonDANN and DANN\"):\n",
        "#     with gr.Row():\n",
        "#       radio_gradcam = gr.Radio([\"Baseline (Non-DANN)\", \"DANN\"],\n",
        "#                                label=\"Select the model you want to use.\",\n",
        "#                                value=\"Baseline (Non-DANN)\",  # Set default value\n",
        "#                                scale=2)\n",
        "\n",
        "#     with gr.Row():\n",
        "#       with gr.Column():\n",
        "#         input_image_classify = gr.Image(label=\"Classify Digit\", type = \"pil\", height = 256, width = 256)\n",
        "#         button_classify = gr.Button(\"Submit to Classify Image\", visible = True)\n",
        "\n",
        "\n",
        "\n",
        "#       with gr.Column():\n",
        "#         label_classify = gr.Label(num_top_classes=10, visible = True)\n",
        "\n",
        "#     with gr.Row():\n",
        "#       # gr.Markdown(\"Select the Image\")\n",
        "#       for batch in mnist_loader:\n",
        "#         images, labels = batch[0],batch[1]\n",
        "#         image_pils = [display_transforms(img) for img in images]\n",
        "#         # print(labels)\n",
        "#         # for img,label in zip (images, labels):\n",
        "#         #   img_pil = display_transforms(img)\n",
        "#         #   # with gr.Column():\n",
        "#         #   gr.Examples([img_pil],inputs=[input_image_classify],label=label.item())\n",
        "#         # break\n",
        "#         gr.Examples(image_pils,inputs=[input_image_classify], label = \"Select an example MNIST Image\")\n",
        "#         break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#   ##########################\n",
        "#   with gr.Tab(\"Introduction\"):\n",
        "#     gr.Markdown(\"## Domain Adaptation in Deep Networks - Demonstration\")\n",
        "#     gr.Markdown(\n",
        "#         '''\n",
        "#         Source - MNIST\n",
        "#         ------\n",
        "#         - The MNIST database (Modified National Institute of Standards and Technology database) is a large collection of handwritten digits.\n",
        "#         - It has a training set of 60,000 examples, and a test set of 10,000 examples.\n",
        "#         - 28 x 28 size\n",
        "#         - 1 channel\n",
        "\n",
        "#         '''\n",
        "#         )\n",
        "#     gr.Markdown(\n",
        "#         '''\n",
        "#         Target - MNIST-M\n",
        "#         -------\n",
        "#         - MNIST-M is created by combining MNIST digits with the patches randomly extracted from color photos of BSDS500 as their background.\n",
        "#         - It contains 59,001 training and 90,001 test images.\n",
        "#         - 28 x 28 size\n",
        "#         - 3 channels\n",
        "#         '''\n",
        "#     )\n",
        "#     with gr.Row():\n",
        "#         with gr.Column():\n",
        "#              image_output = gr.Image(value=display_image(), label = \"source and target\",height = 512, width = 512, show_label = True)\n",
        "#   ################\n",
        "\n",
        "#   # button_classify.click(fn=classify_image, inputs =[input_image_classify,slider_classify_num_classes,checkbox_gradcam_classify,dropdown_gradcam_classify_layer,slider_gradcam_classify_opacity], outputs = [label_classify,gallery_gradcam_classify,gallery_gradcam_classify])\n",
        "\n",
        "\n",
        "#   # radio_gradcam.change(fn=view_gradcam_images, inputs=radio_gradcam, outputs=[slider_gradcam_num_images, dropdown_gradcam_layer,slider_gradcam_opacity,button_gradcam, output_gallery_gradcam])\n",
        "\n",
        "\n",
        "\n",
        "# demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "VaKH7W86bhxq",
        "outputId": "252654bd-8c4d-4f97-8bc2-a8b49ebaf55c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://def602e9e6dc2c3bba.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://def602e9e6dc2c3bba.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Gradio interface setup\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            examples = gr.Examples(\n",
        "                examples=get_mnist_images(),\n",
        "                inputs=[gr.Image(type=\"pil\")],\n",
        "                outputs=[gr.Label(num_top_classes=10, visible=True)],\n",
        "                label=\"Select an image to classify\"\n",
        "            )\n",
        "            input_image = gr.Image(label=\"Classify Digit\", type=\"pil\")\n",
        "            classify_button = gr.Button(\"Classify\")\n",
        "            output_label = gr.Label(num_top_classes=10, visible=True)\n",
        "\n",
        "        def classify_image(image):\n",
        "            prediction = classify_digit(image)\n",
        "            return prediction\n",
        "\n",
        "        classify_button.click(classify_image, inputs=input_image, outputs=output_label)\n",
        "\n",
        "    with gr.Tab(\"Introduction\"):\n",
        "        gr.Markdown(\"## Domain Adaptation in Deep Networks - Demonstration\")\n",
        "        gr.Markdown(\n",
        "            '''\n",
        "            Source - MNIST\n",
        "            ------\n",
        "            - The MNIST database (Modified National Institute of Standards and Technology database) is a large collection of handwritten digits.\n",
        "            - It has a training set of 60,000 examples, and a test set of 10,000 examples.\n",
        "            - 28 x 28 size\n",
        "            - 1 channel\n",
        "\n",
        "            '''\n",
        "        )\n",
        "        gr.Markdown(\n",
        "            '''\n",
        "            Target - MNIST-M\n",
        "            -------\n",
        "            - MNIST-M is created by combining MNIST digits with the patches randomly extracted from color photos of BSDS500 as their background.\n",
        "            - It contains 59,001 training and 90,001 test images.\n",
        "            - 28 x 28 size\n",
        "            - 3 channels\n",
        "            '''\n",
        "        )\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUSOxIIDUNBN"
      },
      "outputs": [],
      "source": [
        "# import gradio as gr\n",
        "\n",
        "# def display_image(image):\n",
        "#     return image\n",
        "\n",
        "# iface = gr.Interface(\n",
        "#     fn=display_image,\n",
        "#     inputs=gr.inputs.Image(type=\"pil\"),\n",
        "#     outputs=gr.outputs.Image(type=\"pil\"),\n",
        "#     title=\"Image Display\",\n",
        "#     description=\"Upload an image to display it.\"\n",
        "# )\n",
        "\n",
        "# iface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "3ZVJv7mYUr5z",
        "outputId": "84edc790-3838-460e-8314-d36c87f50aa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://0d6269534cc6fe1ba0.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://0d6269534cc6fe1ba0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Launch the Gradio app\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "SbR-Ksm9jrvw",
        "outputId": "a42ed9e0-3eb4-4822-a170-732a553e36cf"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Image' object has no attribute 'click'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-54c54fc1a6eb>\u001b[0m in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 gr.Image(value=img, label=path, interactive=True).click(\n\u001b[0m\u001b[1;32m     72\u001b[0m                     \u001b[0mdisplay_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_output\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label_classify\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Image' object has no attribute 'click'"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "\n",
        "# Dummy dataset and dataloader for demonstration\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, transform=None):\n",
        "        self.transform = transform\n",
        "        # Assuming the dataset consists of images and labels\n",
        "        self.data = [\n",
        "            ('/content/mnist-m.JPG', 0)\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        image = Image.open(img_path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label, img_path\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Initialize dataset and dataloader\n",
        "dataset = CustomDataset(transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "# Function to get 10 random images\n",
        "def get_random_images():\n",
        "    data_iter = iter(dataloader)\n",
        "    images, labels, paths = next(data_iter)\n",
        "    images = [transforms.ToPILImage()(img) for img in images]\n",
        "    return images, paths\n",
        "\n",
        "# Dummy model for demonstration\n",
        "def classify_digit(image):\n",
        "    # Replace this with actual model inference code\n",
        "    return \"Predicted Label\"\n",
        "\n",
        "# Initialize 10 random images\n",
        "images, paths = get_random_images()\n",
        "\n",
        "# Create Gradio interface\n",
        "def display_images(image_path):\n",
        "    # Find the corresponding image from paths\n",
        "    selected_image = None\n",
        "    for img, path in zip(images, paths):\n",
        "        if path == image_path:\n",
        "            selected_image = img\n",
        "            break\n",
        "\n",
        "    if selected_image:\n",
        "        return selected_image, classify_digit(selected_image)\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Select an Image and Classify Digit\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # Display 10 random images\n",
        "        for img, path in zip(images, paths):\n",
        "            with gr.Column():\n",
        "                gr.Image(value=img, label=path, interactive=True).click(\n",
        "                    display_images, inputs=path, outputs=[\"image_output\", \"label_classify\"]\n",
        "                )\n",
        "\n",
        "    with gr.Row():\n",
        "        image_output = gr.Image(label=\"Selected Image\")\n",
        "        label_classify = gr.Label(label=\"Classification Result\")\n",
        "\n",
        "# Launch the Gradio app\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from PIL import Image\n",
        "\n",
        "# Define the function to be called with the examples\n",
        "def classify_image(image):\n",
        "    return \"Label for the image\"\n",
        "\n",
        "# Define the examples and their labels\n",
        "examples = [\n",
        "    (\"example1.jpg\", \"Example 1\"),\n",
        "    (\"example2.jpg\", \"Example 2\"),\n",
        "    (\"example3.jpg\", \"Example 3\")\n",
        "]\n",
        "\n",
        "# Create the Gradio Blocks interface\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        # Input component\n",
        "        input_image = gr.Image(type=\"pil\", label=\"Input Image\")\n",
        "\n",
        "        # Output component\n",
        "        output_label = gr.Label(label=\"Output Label\")\n",
        "\n",
        "    # Examples component with custom layout\n",
        "    with gr.Row():\n",
        "        for img_path, img_label in examples:\n",
        "            with gr.Column():\n",
        "                gr.Image(value=img_path, label=img_label)\n",
        "                gr.Markdown(img_label)\n",
        "\n",
        "    # Button to submit the input\n",
        "    submit_button = gr.Button(\"Submit\")\n",
        "\n",
        "    # Define the interaction\n",
        "    submit_button.click(classify_image, inputs=input_image, outputs=output_label)\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "9FeWfwm35Spa",
        "outputId": "9194fddb-c543-4ee2-c339-9cbd49249128"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://c01faec8a4fda3d7c7.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c01faec8a4fda3d7c7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNxOoz1S2utc3zjm7odH140",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}