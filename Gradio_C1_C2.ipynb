{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYJXRSJRe8fMXQMHgAqxuk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scaairesearch/da_demo/blob/main/Gradio_C1_C2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Yo1zNX5SMAe",
        "outputId": "b64a2683-b496-4e1e-f732-776c1d4fb483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.2/318.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m981.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import datasets,transforms\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Function\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "import io\n",
        "import base64"
      ],
      "metadata": {
        "id": "85lFPERcSW6D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the mounted drive and mounting if not done\n",
        "if not os.path.exists('/content/gdrive'):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "else:\n",
        "    print(\"Google Drive is already mounted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_mD3B5GSWAi",
        "outputId": "d2216858-911e-49fa-dbc9-34d93025c85f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_c1 = torch.load('/content/gdrive/MyDrive/da_demo/cv/models/26_06/list_mnist_m_non_dann_misclassified_dann_classified.pt')"
      ],
      "metadata": {
        "id": "3sFGkk9ETJXx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        imgs, labels, image_names = self.data[idx]\n",
        "        return imgs, labels, image_names\n",
        "\n",
        "dataset_c1 = CustomDataset(list_c1)"
      ],
      "metadata": {
        "id": "cz_VFRRHTPeB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataloader with the filtered dataset\n",
        "dataloader_c1 = torch.utils.data.DataLoader(dataset_c1, batch_size=10, shuffle=True)"
      ],
      "metadata": {
        "id": "O2fivGGNTgxZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_to_pil  = transforms.ToPILImage()\n",
        "\n",
        "def get_images():\n",
        "    images, labels,image_names = next(iter(dataloader_c1))\n",
        "    pil_images = [transform_to_pil(image) for image in images]\n",
        "    return pil_images, labels.tolist()\n",
        "\n"
      ],
      "metadata": {
        "id": "WK3mm4avUFKq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_c2 = torch.load('/content/gdrive/MyDrive/da_demo/cv/models/26_06/list_mnist_m_non_dann_misclassified_dann_misclassified.pt')\n",
        "dataset_c2 = CustomDataset(list_c2)\n",
        "dataloader_c2 = torch.utils.data.DataLoader(dataset_c2, batch_size=10, shuffle=True)\n",
        "def get_images_2():\n",
        "    images, labels,image_names = next(iter(dataloader_c2))\n",
        "    pil_images = [transform_to_pil(image) for image in images]\n",
        "    return pil_images, labels.tolist()"
      ],
      "metadata": {
        "id": "5v2X9kpjX704"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# next(iter(dataloader_c1))"
      ],
      "metadata": {
        "id": "_XSX72hwTqjA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "    print(\"Device Selected:\", device)\n",
        "    return device\n",
        "\n",
        "device = get_device()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egIXDJXXVh6h",
        "outputId": "4ea2df69-c7b9-45b9-b148-862b88c1bb20"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device Selected: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GradientReversalFn(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        return output, None\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, num_classes = 10):\n",
        "        super(Network, self).__init__()  # Initialize the parent class\n",
        "\n",
        "        drop_out_value = 0.1\n",
        "\n",
        "        #---------------------Feature Extractor Network------------------------#\n",
        "        self.feature_extractor  = nn.Sequential(\n",
        "            # Input Block\n",
        "            nn.Conv2d(3, 16, 3, bias=False),  # In: 3x28x28, Out: 16x26x26, RF: 3x3, Stride: 1\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(drop_out_value),\n",
        "\n",
        "            # Conv Block 2\n",
        "            nn.Conv2d(16, 16, 3, bias=False),  # In: 16x26x26, Out: 16x24x24, RF: 5x5, Stride: 1\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(drop_out_value),\n",
        "\n",
        "            # Conv Block 3\n",
        "            nn.Conv2d(16, 16, 3, bias=False),  # In: 16x24x24, Out: 16x22x22, RF: 7x7, Stride: 1\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(drop_out_value),\n",
        "\n",
        "            # Transition Block 1\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # In: 16x22x22, Out: 16x11x11, RF: 8x8, Stride: 2\n",
        "\n",
        "            # Conv Block 4\n",
        "            nn.Conv2d(16, 16, 3, bias=False),  # In: 16x11x11, Out: 16x9x9, RF: 12x12, Stride: 1\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(drop_out_value),\n",
        "\n",
        "            # Conv Block 5\n",
        "            nn.Conv2d(16, 32, 3, bias=False),  # In: 16x9x9, Out: 32x7x7, RF: 16x16, Stride: 1\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(drop_out_value),\n",
        "\n",
        "            # Output Block\n",
        "            nn.Conv2d(32, 64, 1, bias=False),  # In: 32x7x7, Out: 64x7x7, RF: 16x16, Stride: 1\n",
        "\n",
        "            # Global Average Pooling\n",
        "            nn.AvgPool2d(7)  # In: 64x7x7, Out: 64x1x1, RF: 16x16, Stride: 7\n",
        "        )\n",
        "\n",
        "        #---------------------Class Classifier Network------------------------#\n",
        "        self.class_classifier = nn.Sequential(nn.ReLU(),\n",
        "                                        nn.Dropout(p=drop_out_value),\n",
        "                                        nn.Linear(64,50),\n",
        "                                        nn.BatchNorm1d(50), # added batch norm to improve accuracy\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Dropout(p=drop_out_value),\n",
        "                                        nn.Linear(50,num_classes))\n",
        "\n",
        "        #---------------------Label Classifier Network------------------------#\n",
        "        self.domain_classifier = nn.Sequential(nn.ReLU(),\n",
        "                                        nn.Dropout(p=drop_out_value),\n",
        "                                        nn.Linear(64,50),\n",
        "                                        nn.BatchNorm1d(50), # added batch norm to improve accuracy\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Dropout(p=drop_out_value),\n",
        "                                        nn.Linear(50,2))\n",
        "    def forward(self, input_data, alpha = 1.0):\n",
        "      if input_data.data.shape[1] == 1:\n",
        "        input_data = input_data.expand(input_data.data.shape[0], 3, img_size, img_size)\n",
        "\n",
        "      input_data = self.feature_extractor(input_data)\n",
        "\n",
        "      features = input_data.view(input_data.size(0), -1)  # Flatten the output for fully connected layer\n",
        "\n",
        "      reverse_features = GradientReversalFn.apply(features, alpha)\n",
        "      class_output = self.class_classifier(features)\n",
        "      domain_output = self.domain_classifier(reverse_features)\n",
        "\n",
        "      return class_output, domain_output, features"
      ],
      "metadata": {
        "id": "3bhvho_HVn5R"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## NON DANN\n",
        "# Instantiate the model (make sure it has the same architecture)\n",
        "loaded_model_non_dann = Network()\n",
        "loaded_model_non_dann = loaded_model_non_dann.to(device)\n",
        "# Load the saved state dictionary\n",
        "loaded_model_non_dann.load_state_dict(torch.load('/content/gdrive/MyDrive/da_demo/cv/models/26_06/non_dann_26_06.pt', map_location=device), strict=False)\n",
        "loaded_model_non_dann.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC7wlRi-Vqzj",
        "outputId": "e81f297f-7362-400d-c432-170e5d216e20"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (feature_extractor): Sequential(\n",
              "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Dropout(p=0.1, inplace=False)\n",
              "    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (5): ReLU()\n",
              "    (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): Dropout(p=0.1, inplace=False)\n",
              "    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (9): ReLU()\n",
              "    (10): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): Dropout(p=0.1, inplace=False)\n",
              "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (13): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (14): ReLU()\n",
              "    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (16): Dropout(p=0.1, inplace=False)\n",
              "    (17): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (18): ReLU()\n",
              "    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (20): Dropout(p=0.1, inplace=False)\n",
              "    (21): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (22): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
              "  )\n",
              "  (class_classifier): Sequential(\n",
              "    (0): ReLU()\n",
              "    (1): Dropout(p=0.1, inplace=False)\n",
              "    (2): Linear(in_features=64, out_features=50, bias=True)\n",
              "    (3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.1, inplace=False)\n",
              "    (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "  )\n",
              "  (domain_classifier): Sequential(\n",
              "    (0): ReLU()\n",
              "    (1): Dropout(p=0.1, inplace=False)\n",
              "    (2): Linear(in_features=64, out_features=50, bias=True)\n",
              "    (3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.1, inplace=False)\n",
              "    (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##  DANN\n",
        "# Instantiate the model (make sure it has the same architecture)\n",
        "loaded_model_dann = Network()\n",
        "loaded_model_dann = loaded_model_dann.to(device)\n",
        "# Load the saved state dictionary\n",
        "loaded_model_dann.load_state_dict(torch.load('/content/gdrive/MyDrive/da_demo/cv/models/26_06/dann_26_06.pt', map_location=device), strict=False)\n",
        "loaded_model_dann.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgckA2v3Vyhc",
        "outputId": "89bd8c75-9aac-4d61-c15c-6ce02e4e9971"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (feature_extractor): Sequential(\n",
              "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Dropout(p=0.1, inplace=False)\n",
              "    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (5): ReLU()\n",
              "    (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): Dropout(p=0.1, inplace=False)\n",
              "    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (9): ReLU()\n",
              "    (10): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): Dropout(p=0.1, inplace=False)\n",
              "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (13): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (14): ReLU()\n",
              "    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (16): Dropout(p=0.1, inplace=False)\n",
              "    (17): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (18): ReLU()\n",
              "    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (20): Dropout(p=0.1, inplace=False)\n",
              "    (21): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (22): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
              "  )\n",
              "  (class_classifier): Sequential(\n",
              "    (0): ReLU()\n",
              "    (1): Dropout(p=0.1, inplace=False)\n",
              "    (2): Linear(in_features=64, out_features=50, bias=True)\n",
              "    (3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.1, inplace=False)\n",
              "    (6): Linear(in_features=50, out_features=10, bias=True)\n",
              "  )\n",
              "  (domain_classifier): Sequential(\n",
              "    (0): ReLU()\n",
              "    (1): Dropout(p=0.1, inplace=False)\n",
              "    (2): Linear(in_features=64, out_features=50, bias=True)\n",
              "    (3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.1, inplace=False)\n",
              "    (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = 28 # for mnist\n",
        "cpu_batch_size = 10\n",
        "class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ],
      "metadata": {
        "id": "hfKMeD_AWFyo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_image(image,\n",
        "                   select_model\n",
        "                   ):\n",
        "  if select_model == \"Baseline (Non-DANN)\":\n",
        "    model = loaded_model_non_dann\n",
        "    test_transforms = transforms.Compose([\n",
        "                                       transforms.Resize(img_size),\n",
        "                                       transforms.ToTensor(),# converts to tesnor\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n",
        "    transformed_image = test_transforms(image)\n",
        "    image_tensor = transformed_image.to(device).unsqueeze(0)\n",
        "  if select_model == \"DANN\":\n",
        "    model = loaded_model_dann\n",
        "    target_test_transforms = transforms.Compose([\n",
        "                                       transforms.Resize(img_size),\n",
        "                                       transforms.ToTensor(),# converts to tesnor\n",
        "                                       transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "                                       ])\n",
        "    target_transformed_image = target_test_transforms(image)\n",
        "    image_tensor = target_transformed_image.to(device).unsqueeze(0)\n",
        "\n",
        "\n",
        "  logits,_,_ = model(image_tensor)\n",
        "  output = F.softmax(logits.view(-1), dim = -1) #F.softmax(output.flatten(), dim=-1) #\n",
        "\n",
        "  confidences = [(class_names[i], float(output[i])) for i in range(len(class_names))]\n",
        "  confidences.sort(key=lambda x: x[1], reverse=True)\n",
        "  confidences = OrderedDict(confidences[:3])\n",
        "  label = torch.argmax(output).item()\n",
        "\n",
        "  return confidences\n"
      ],
      "metadata": {
        "id": "bRE0OgJ2VRz8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_image_both(image,\n",
        "                        transforms = None):\n",
        "  if transforms == None:\n",
        "    target_test_transforms = transforms.Compose([\n",
        "                                      transforms.Resize(img_size),\n",
        "                                      transforms.ToTensor(),# converts to tesnor\n",
        "                                      transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "                                      ])\n",
        "  else:\n",
        "    target_test_transforms = transforms\n",
        "\n",
        "  target_transformed_image = target_test_transforms(image)\n",
        "  image_tensor = target_transformed_image.to(device).unsqueeze(0)\n",
        "\n",
        "  list_confidences = []\n",
        "  for model in [loaded_model_non_dann, loaded_model_dann]:\n",
        "    model.eval()\n",
        "    logits,_,_ = model(image_tensor)\n",
        "    output = F.softmax(logits.view(-1), dim = -1)\n",
        "\n",
        "    confidences = [(class_names[i], float(output[i])) for i in range(len(class_names))]\n",
        "    confidences.sort(key=lambda x: x[1], reverse=True)\n",
        "    confidences = OrderedDict(confidences[:3])\n",
        "    label = torch.argmax(output).item()\n",
        "    list_confidences.append(confidences)\n",
        "\n",
        "\n",
        "  return list_confidences[0],list_confidences[1]"
      ],
      "metadata": {
        "id": "UDmoRaMKZH0C"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### SOURCE DATA - MNIST\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize(img_size),\n",
        "                                       transforms.ToTensor(),# converts to tesnor\n",
        "                                      #  transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n",
        "transform_to_pil  = transforms.ToPILImage()\n",
        "test = datasets.MNIST('./data',\n",
        "                      train=False,\n",
        "                      download=True,\n",
        "                      transform=test_transforms)\n",
        "\n",
        "dataloader_args = dict(shuffle=True, batch_size=cpu_batch_size)\n",
        "\n",
        "mnist_loader = torch.utils.data.DataLoader(\n",
        "    dataset = test,\n",
        "    **dataloader_args\n",
        ")\n",
        "\n",
        "def get_mnist_images():\n",
        "    images, labels = next(iter(mnist_loader))\n",
        "    pil_images = [transform_to_pil(image) for image in images]\n",
        "    return pil_images, labels.tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhk-3oNciLXX",
        "outputId": "2545a0b2-579e-4fc0-e0ab-8f198df70060"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 22031314.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 623873.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 5526439.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3859507.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splits = {'train': 'data/train-00000-of-00001-571b6b1e2c195186.parquet', 'test': 'data/test-00000-of-00001-ba3ad971b105ff65.parquet'}\n",
        "df = pd.read_parquet(\"hf://datasets/Mike0307/MNIST-M/\" + splits[\"test\"])\n",
        "\n",
        "class MNIST_M(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "      self.dataframe = dataframe\n",
        "      self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image and label from dataframe\n",
        "        img_data = self.dataframe.iloc[idx]['image']['bytes']\n",
        "        label = self.dataframe.iloc[idx]['label']\n",
        "        img_path = self.dataframe.iloc[idx]['image']['path']\n",
        "\n",
        "        # Decode image data (assuming it's base64 encoded)\n",
        "        img = Image.open(io.BytesIO(img_data))\n",
        "\n",
        "        # img = Image.open(io.BytesIO(base64.b64decode(img_data)))\n",
        "\n",
        "        # Apply transformations if any\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label,img_path\n",
        "\n",
        "\n",
        "# Test Phase transformations\n",
        "target_test_transforms = transforms.Compose([\n",
        "                                       transforms.Resize(img_size),\n",
        "                                       transforms.ToTensor(),# converts to tesnor\n",
        "                                       transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "                                       ])\n",
        "\n",
        "\n",
        "transform_to_pil  = transforms.ToPILImage()\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "target_test_dataset = MNIST_M(dataframe=df, transform=target_test_transforms)\n",
        "target_test_dataloader = torch.utils.data.DataLoader(target_test_dataset, batch_size=cpu_batch_size, shuffle=True)\n",
        "def get_mnist_m_images():\n",
        "    images, labels,image_names = next(iter(target_test_dataloader))\n",
        "    pil_images = [transform_to_pil(image) for image in images]\n",
        "    return pil_images, labels.tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osd6OLLjnqY1",
        "outputId": "68c40978-4ecd-41ff-eb6b-b5c17ca5b367"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_images, mnist_labels = get_mnist_images()\n",
        "mnist_m_images,mnist_m_labels = get_mnist_m_images()"
      ],
      "metadata": {
        "id": "xoIqN11Djv-r"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_image_inference(image):\n",
        "  # print(image.mode)\n",
        "  image_transforms = None\n",
        "  if image.mode == \"L\":\n",
        "    # image = image.convert(\"RGB\")\n",
        "    source = 'MNIST'\n",
        "    image_transforms = transforms.Compose([\n",
        "                                  transforms.Resize(img_size),\n",
        "                                  transforms.ToTensor(),# converts to tesnor\n",
        "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                  ])\n",
        "  else:\n",
        "    source = 'MNIST-M'\n",
        "    image_transforms = transforms.Compose([\n",
        "                            transforms.Resize(img_size),\n",
        "                            transforms.ToTensor(),# converts to tesnor\n",
        "                            transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "                            ])\n",
        "\n",
        "  return classify_image_both(image, transforms = image_transforms)\n"
      ],
      "metadata": {
        "id": "Lx01KkBnhx_N"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_image():\n",
        "    # Load the image from a local file\n",
        "    image = Image.open(\"/content/gdrive/MyDrive/da_demo/mnist-m.JPG\")\n",
        "    return image"
      ],
      "metadata": {
        "id": "qKUR7c67qP-g"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "  with gr.Tab(\"Introduction\"):\n",
        "      gr.Markdown(\"## Domain Adaptation in Deep Networks - Demonstration\")\n",
        "      gr.Markdown(\n",
        "          '''\n",
        "          Source - MNIST\n",
        "          ------\n",
        "          - The MNIST database (Modified National Institute of Standards and Technology database) is a large collection of handwritten digits.\n",
        "          - It has a training set of 60,000 examples, and a test set of 10,000 examples.\n",
        "          - 28 x 28 size\n",
        "          - 1 channel\n",
        "\n",
        "          '''\n",
        "          )\n",
        "      gr.Markdown(\n",
        "          '''\n",
        "          Target - MNIST-M\n",
        "          -------\n",
        "          - MNIST-M is created by combining MNIST digits with the patches randomly extracted from color photos of BSDS500 as their background.\n",
        "          - It contains 59,001 training and 90,001 test images.\n",
        "          - 28 x 28 size\n",
        "          - 3 channels\n",
        "          '''\n",
        "      )\n",
        "      with gr.Row():\n",
        "          with gr.Column():\n",
        "              image_output = gr.Image(value=display_image(), label = \"source and target\",height = 512, width = 512, show_label = True)\n",
        "\n",
        "################################################\n",
        "  with gr.Tab(\"Inferencing on NonDANN and DANN\"):\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        input_image_classify_mnist = gr.Image(label=\"Classify MNIST Digit\", type = \"pil\", height = 256, width = 256, image_mode = 'L')\n",
        "        button_classify_mnist = gr.Button(\"Submit to Classify MNIST Image\", visible = True, size ='sm')\n",
        "      with gr.Column():\n",
        "        with gr.Row():\n",
        "          label_classify_mnist_non_dann = gr.Label(label = \"NON DANN Predicted MNIST label\", num_top_classes=2, visible = True)\n",
        "        with gr.Row():\n",
        "          label_classify_mnist_dann = gr.Label(label = \"DANN Predicted MNIST label\", num_top_classes=2, visible = True)\n",
        "    with gr.Row():\n",
        "      gr.Examples( [img.convert(\"L\") for img in mnist_images],\n",
        "                  inputs=[input_image_classify_mnist], label = \"Select an example MNIST Image\")\n",
        "\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        input_image_classify_mnist_m = gr.Image(label=\"Classify MNIST M Digit\", type = \"pil\", height = 256, width = 256, image_mode = 'RGB')\n",
        "        button_classify_mnist_m = gr.Button(\"Submit to Classify MNIST M Image\", visible = True, size ='sm')\n",
        "      with gr.Column():\n",
        "        with gr.Row():\n",
        "          label_classify_mnist_m_non_dann = gr.Label(label = \"NON DANN Predicted MNIST M label\", num_top_classes=2, visible = True)\n",
        "        with gr.Row():\n",
        "          label_classify_mnist_m_dann = gr.Label(label = \"DANN Predicted MNIST M label\", num_top_classes=2, visible = True)\n",
        "    with gr.Row():\n",
        "      gr.Examples( [img.convert(\"RGB\") for img in mnist_m_images],\n",
        "                  inputs=[input_image_classify_mnist_m], label = \"Select an example MNIST M Image\")\n",
        "    with gr.Row():\n",
        "      gr.Markdown(value = f'MNIST- M Ground Truth Label = {[label for label in mnist_m_labels]}')\n",
        "\n",
        "    button_classify_mnist.click(fn=classify_image_inference,\n",
        "                          inputs=[input_image_classify_mnist],\n",
        "                          outputs=[label_classify_mnist_non_dann, label_classify_mnist_dann])\n",
        "\n",
        "    button_classify_mnist_m.click(fn=classify_image_inference,\n",
        "                          inputs=[input_image_classify_mnist_m],\n",
        "                          outputs=[label_classify_mnist_m_non_dann, label_classify_mnist_m_dann])\n",
        "\n",
        "\n",
        "  ######################\n",
        "  with gr.Tab(\"Case 1: MNIST_M_Non_DANN_Misclassify_DANN_Classify\"):\n",
        "    with gr.Row():\n",
        "      radio_model = gr.Radio([\"Baseline (Non-DANN)\", \"DANN\"],\n",
        "                               label=\"Select the model you want to use.\",\n",
        "                               value=\"Baseline (Non-DANN)\",  # Set default value\n",
        "                               scale=2)\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        input_image_classify = gr.Image(label=\"Classify Digit\", type = \"pil\", height = 256, width = 256)\n",
        "        button_classify = gr.Button(\"Submit to Classify Image\", visible = True, size ='sm')\n",
        "\n",
        "      with gr.Column():\n",
        "        with gr.Row():\n",
        "          label_classify = gr.Label(label = \"Predicted label\", num_top_classes=10, visible = True)\n",
        "\n",
        "    mnist_m_images,mnist_m_labels = get_images()\n",
        "\n",
        "    with gr.Row():\n",
        "      gr.Examples(mnist_m_images,inputs=[input_image_classify], label = \"Select an example MNIST-M Image\") #working\n",
        "\n",
        "    with gr.Row():\n",
        "      gr.Markdown(value = f'MNIST- M Ground Truth Label = {[label for label in mnist_m_labels]}')\n",
        "\n",
        "\n",
        "    button_classify.click(fn=classify_image,\n",
        "                          inputs=[input_image_classify,radio_model],\n",
        "                          outputs=[label_classify])\n",
        "########################################################################\n",
        "  with gr.Tab(\"Case 2: MNIST_M_Both_Misclassify\"):\n",
        "    with gr.Row():\n",
        "      radio_model = gr.Radio([\"Baseline (Non-DANN)\", \"DANN\"],\n",
        "                               label=\"Select the model you want to use.\",\n",
        "                               value=\"Baseline (Non-DANN)\",  # Set default value\n",
        "                               scale=2)\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        input_image_classify = gr.Image(label=\"Classify Digit\", type = \"pil\", height = 256, width = 256)\n",
        "        button_classify = gr.Button(\"Submit to Classify Image\", visible = True, size ='sm')\n",
        "\n",
        "      with gr.Column():\n",
        "        with gr.Row():\n",
        "          label_classify = gr.Label(label = \"Predicted label\", num_top_classes=10, visible = True)\n",
        "\n",
        "    mnist_m_images,mnist_m_labels = get_images_2()\n",
        "\n",
        "    with gr.Row():\n",
        "      gr.Examples(mnist_m_images,inputs=[input_image_classify], label = \"Select an example MNIST-M Image\") #working\n",
        "\n",
        "    with gr.Row():\n",
        "      gr.Markdown(value = f'MNIST- M Ground Truth Label = {[label for label in mnist_m_labels]}')\n",
        "\n",
        "\n",
        "    button_classify.click(fn=classify_image,\n",
        "                          inputs=[input_image_classify,radio_model],\n",
        "                          outputs=[label_classify])\n",
        "########################################################################\n",
        "\n",
        "  with gr.Tab(\"Case 2 - Show both: MNIST_M_Both_Misclassify\"):\n",
        "\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        input_image_classify_both = gr.Image(label=\"Classify Digit\", type = \"pil\", height = 256, width = 256)\n",
        "        button_classify_both = gr.Button(\"Submit to Classify Image with Both Models\", visible = True, size ='sm')\n",
        "\n",
        "      with gr.Column():\n",
        "        with gr.Row():\n",
        "          label_classify_non_dann = gr.Label(label = \"NON DANN Predicted label\", num_top_classes=3, visible = True)\n",
        "        with gr.Row():\n",
        "          label_classify_dann = gr.Label(label = \"DANN Predicted label\", num_top_classes=3, visible = True)\n",
        "\n",
        "    mnist_m_images,mnist_m_labels = get_images_2()\n",
        "\n",
        "    with gr.Row():\n",
        "      gr.Examples(mnist_m_images,inputs=[input_image_classify_both], label = \"Select an example MNIST-M Image\") #working\n",
        "\n",
        "    with gr.Row():\n",
        "      gr.Markdown(value = f'MNIST- M Ground Truth Label = {[label for label in mnist_m_labels]}')\n",
        "\n",
        "\n",
        "    button_classify_both.click(fn=classify_image_both,\n",
        "                          inputs=[input_image_classify_both],\n",
        "                          outputs=[label_classify_non_dann,label_classify_dann])\n",
        "\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "EoglGKs0Skaw",
        "outputId": "0d609036-59d8-4aee-ecf6-d12746a072e2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://e5847bd30c2328417f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e5847bd30c2328417f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://e5847bd30c2328417f.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}