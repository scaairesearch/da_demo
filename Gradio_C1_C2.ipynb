{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODv02OLbqRTDqJrYG2lZGi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scaairesearch/da_demo/blob/main/Gradio_C1_C2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Yo1zNX5SMAe",
        "outputId": "4dda2c0e-c36f-4f81-c3fb-21c552f08b36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.2/318.2 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import datasets,transforms\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Function\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "import io\n",
        "import base64"
      ],
      "metadata": {
        "id": "85lFPERcSW6D"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the mounted drive and mounting if not done\n",
        "if not os.path.exists('/content/gdrive'):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "else:\n",
        "    print(\"Google Drive is already mounted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_mD3B5GSWAi",
        "outputId": "b1b4be5b-880b-4448-a379-4450a9474203"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_c1 = torch.load('/content/gdrive/MyDrive/da_demo/cv/models/26_06/list_mnist_m_non_dann_misclassified_dann_classified.pt')"
      ],
      "metadata": {
        "id": "3sFGkk9ETJXx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        imgs, labels, image_names = self.data[idx]\n",
        "        return imgs, labels, image_names\n",
        "\n",
        "dataset_c1 = CustomDataset(list_c1)"
      ],
      "metadata": {
        "id": "cz_VFRRHTPeB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataloader with the filtered dataset\n",
        "dataloader_c1 = torch.utils.data.DataLoader(dataset_c1, batch_size=10, shuffle=True)"
      ],
      "metadata": {
        "id": "O2fivGGNTgxZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_to_pil  = transforms.ToPILImage()\n",
        "\n",
        "def get_images():\n",
        "    images, labels,image_names = next(iter(dataloader_c1))\n",
        "    pil_images = [transform_to_pil(image) for image in images]\n",
        "    return pil_images, labels.tolist()\n",
        "\n"
      ],
      "metadata": {
        "id": "WK3mm4avUFKq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_c2 = torch.load('/content/gdrive/MyDrive/da_demo/cv/models/26_06/list_mnist_m_non_dann_misclassified_dann_misclassified.pt')\n",
        "dataset_c2 = CustomDataset(list_c2)\n",
        "dataloader_c2 = torch.utils.data.DataLoader(dataset_c2, batch_size=10, shuffle=True)\n",
        "def get_images_2():\n",
        "    images, labels,image_names = next(iter(dataloader_c2))\n",
        "    pil_images = [transform_to_pil(image) for image in images]\n",
        "    return pil_images, labels.tolist()"
      ],
      "metadata": {
        "id": "5v2X9kpjX704"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# next(iter(dataloader_c1))"
      ],
      "metadata": {
        "id": "_XSX72hwTqjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "    print(\"Device Selected:\", device)\n",
        "    return device\n",
        "\n",
        "device = get_device()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egIXDJXXVh6h",
        "outputId": "913b9ea3-4970-4a72-83ff-b5d4cacdf33e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device Selected: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GradientReversalFn(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        return output, None\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, num_classes = 10):\n",
        "        super(Network, self).__init__()  # Initialize the parent class\n",
        "\n",
        "        drop_out_value = 0.1\n",
        "\n",
        "        #---------------------Feature Extractor Network------------------------#\n",
        "        self.feature_extractor  = nn.Sequential(\n",
        "            # Input Block\n",
        "            nn.Conv2d(3, 16, 3, bias=False),  # In: 3x28x28, Out: 16x26x26, RF: 3x3, Stride: 1\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(drop_out_value),\n",
        "\n",
        "            # Conv Block 2\n",
        "            nn.Conv2d(16, 16, 3, bias=False),  # In: 16x26x26, Out: 16x24x24, RF: 5x5, Stride: 1\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(drop_out_value),\n",
        "\n",
        "            # Conv Block 3\n",
        "            nn.Conv2d(16, 16, 3, bias=False),  # In: 16x24x24, Out: 16x22x22, RF: 7x7, Stride: 1\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(drop_out_value),\n",
        "\n",
        "            # Transition Block 1\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # In: 16x22x22, Out: 16x11x11, RF: 8x8, Stride: 2\n",
        "\n",
        "            # Conv Block 4\n",
        "            nn.Conv2d(16, 16, 3, bias=False),  # In: 16x11x11, Out: 16x9x9, RF: 12x12, Stride: 1\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(drop_out_value),\n",
        "\n",
        "            # Conv Block 5\n",
        "            nn.Conv2d(16, 32, 3, bias=False),  # In: 16x9x9, Out: 32x7x7, RF: 16x16, Stride: 1\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(drop_out_value),\n",
        "\n",
        "            # Output Block\n",
        "            nn.Conv2d(32, 64, 1, bias=False),  # In: 32x7x7, Out: 64x7x7, RF: 16x16, Stride: 1\n",
        "\n",
        "            # Global Average Pooling\n",
        "            nn.AvgPool2d(7)  # In: 64x7x7, Out: 64x1x1, RF: 16x16, Stride: 7\n",
        "        )\n",
        "\n",
        "        #---------------------Class Classifier Network------------------------#\n",
        "        self.class_classifier = nn.Sequential(nn.ReLU(),\n",
        "                                        nn.Dropout(p=drop_out_value),\n",
        "                                        nn.Linear(64,50),\n",
        "                                        nn.BatchNorm1d(50), # added batch norm to improve accuracy\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Dropout(p=drop_out_value),\n",
        "                                        nn.Linear(50,num_classes))\n",
        "\n",
        "        #---------------------Label Classifier Network------------------------#\n",
        "        self.domain_classifier = nn.Sequential(nn.ReLU(),\n",
        "                                        nn.Dropout(p=drop_out_value),\n",
        "                                        nn.Linear(64,50),\n",
        "                                        nn.BatchNorm1d(50), # added batch norm to improve accuracy\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Dropout(p=drop_out_value),\n",
        "                                        nn.Linear(50,2))\n",
        "    def forward(self, input_data, alpha = 1.0):\n",
        "      if input_data.data.shape[1] == 1:\n",
        "        input_data = input_data.expand(input_data.data.shape[0], 3, img_size, img_size)\n",
        "\n",
        "      input_data = self.feature_extractor(input_data)\n",
        "\n",
        "      features = input_data.view(input_data.size(0), -1)  # Flatten the output for fully connected layer\n",
        "\n",
        "      reverse_features = GradientReversalFn.apply(features, alpha)\n",
        "      class_output = self.class_classifier(features)\n",
        "      domain_output = self.domain_classifier(reverse_features)\n",
        "\n",
        "      return class_output, domain_output, features"
      ],
      "metadata": {
        "id": "3bhvho_HVn5R"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## NON DANN\n",
        "# Instantiate the model (make sure it has the same architecture)\n",
        "loaded_model_non_dann = Network()\n",
        "loaded_model_non_dann = loaded_model_non_dann.to(device)\n",
        "# Load the saved state dictionary\n",
        "loaded_model_non_dann.load_state_dict(torch.load('/content/gdrive/MyDrive/da_demo/cv/models/26_06/non_dann_26_06.pt', map_location=device), strict=False)\n",
        "loaded_model_non_dann.eval()"
      ],
      "metadata": {
        "id": "GC7wlRi-Vqzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##  DANN\n",
        "# Instantiate the model (make sure it has the same architecture)\n",
        "loaded_model_dann = Network()\n",
        "loaded_model_dann = loaded_model_dann.to(device)\n",
        "# Load the saved state dictionary\n",
        "loaded_model_dann.load_state_dict(torch.load('/content/gdrive/MyDrive/da_demo/cv/models/26_06/dann_26_06.pt', map_location=device), strict=False)\n",
        "loaded_model_dann.eval()"
      ],
      "metadata": {
        "id": "NgckA2v3Vyhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = 28 # for mnist\n",
        "cpu_batch_size = 10\n",
        "class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ],
      "metadata": {
        "id": "hfKMeD_AWFyo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_image(image,\n",
        "                   select_model\n",
        "                   ):\n",
        "  if select_model == \"Baseline (Non-DANN)\":\n",
        "    model = loaded_model_non_dann\n",
        "    test_transforms = transforms.Compose([\n",
        "                                       transforms.Resize(img_size),\n",
        "                                       transforms.ToTensor(),# converts to tesnor\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n",
        "    transformed_image = test_transforms(image)\n",
        "    image_tensor = transformed_image.to(device).unsqueeze(0)\n",
        "  if select_model == \"DANN\":\n",
        "    model = loaded_model_dann\n",
        "    target_test_transforms = transforms.Compose([\n",
        "                                       transforms.Resize(img_size),\n",
        "                                       transforms.ToTensor(),# converts to tesnor\n",
        "                                       transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "                                       ])\n",
        "    target_transformed_image = target_test_transforms(image)\n",
        "    image_tensor = target_transformed_image.to(device).unsqueeze(0)\n",
        "\n",
        "\n",
        "  logits,_,_ = model(image_tensor)\n",
        "  output = F.softmax(logits.view(-1), dim = -1) #F.softmax(output.flatten(), dim=-1) #\n",
        "\n",
        "  confidences = [(class_names[i], float(output[i])) for i in range(len(class_names))]\n",
        "  confidences.sort(key=lambda x: x[1], reverse=True)\n",
        "  confidences = OrderedDict(confidences[:3])\n",
        "  label = torch.argmax(output).item()\n",
        "\n",
        "  return confidences\n"
      ],
      "metadata": {
        "id": "bRE0OgJ2VRz8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_image_both(image\n",
        "                   ):\n",
        "  target_test_transforms = transforms.Compose([\n",
        "                                    transforms.Resize(img_size),\n",
        "                                    transforms.ToTensor(),# converts to tesnor\n",
        "                                    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "                                    ])\n",
        "  target_transformed_image = target_test_transforms(image)\n",
        "  image_tensor = target_transformed_image.to(device).unsqueeze(0)\n",
        "\n",
        "  list_confidences = []\n",
        "  for model in [loaded_model_non_dann, loaded_model_dann]:\n",
        "    model.eval()\n",
        "    logits,_,_ = model(image_tensor)\n",
        "    output = F.softmax(logits.view(-1), dim = -1)\n",
        "\n",
        "    confidences = [(class_names[i], float(output[i])) for i in range(len(class_names))]\n",
        "    confidences.sort(key=lambda x: x[1], reverse=True)\n",
        "    confidences = OrderedDict(confidences[:3])\n",
        "    label = torch.argmax(output).item()\n",
        "    list_confidences.append(confidences)\n",
        "\n",
        "\n",
        "  return list_confidences[0],list_confidences[1]"
      ],
      "metadata": {
        "id": "UDmoRaMKZH0C"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "  with gr.Tab(\"Case 1: MNIST_M_Non_DANN_Misclassify_DANN_Classify\"):\n",
        "    with gr.Row():\n",
        "      radio_model = gr.Radio([\"Baseline (Non-DANN)\", \"DANN\"],\n",
        "                               label=\"Select the model you want to use.\",\n",
        "                               value=\"Baseline (Non-DANN)\",  # Set default value\n",
        "                               scale=2)\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        input_image_classify = gr.Image(label=\"Classify Digit\", type = \"pil\", height = 256, width = 256)\n",
        "        button_classify = gr.Button(\"Submit to Classify Image\", visible = True, size ='sm')\n",
        "\n",
        "      with gr.Column():\n",
        "        with gr.Row():\n",
        "          label_classify = gr.Label(label = \"Predicted label\", num_top_classes=10, visible = True)\n",
        "\n",
        "    mnist_m_images,mnist_m_labels = get_images()\n",
        "\n",
        "    with gr.Row():\n",
        "      gr.Examples(mnist_m_images,inputs=[input_image_classify], label = \"Select an example MNIST-M Image\") #working\n",
        "\n",
        "    with gr.Row():\n",
        "      gr.Markdown(value = f'MNIST- M Ground Truth Label = {[label for label in mnist_m_labels]}')\n",
        "\n",
        "\n",
        "    button_classify.click(fn=classify_image,\n",
        "                          inputs=[input_image_classify,radio_model],\n",
        "                          outputs=[label_classify])\n",
        "\n",
        "  with gr.Tab(\"Case 2: MNIST_M_Both_Misclassify\"):\n",
        "    with gr.Row():\n",
        "      radio_model = gr.Radio([\"Baseline (Non-DANN)\", \"DANN\"],\n",
        "                               label=\"Select the model you want to use.\",\n",
        "                               value=\"Baseline (Non-DANN)\",  # Set default value\n",
        "                               scale=2)\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        input_image_classify = gr.Image(label=\"Classify Digit\", type = \"pil\", height = 256, width = 256)\n",
        "        button_classify = gr.Button(\"Submit to Classify Image\", visible = True, size ='sm')\n",
        "\n",
        "      with gr.Column():\n",
        "        with gr.Row():\n",
        "          label_classify = gr.Label(label = \"Predicted label\", num_top_classes=10, visible = True)\n",
        "\n",
        "    mnist_m_images,mnist_m_labels = get_images_2()\n",
        "\n",
        "    with gr.Row():\n",
        "      gr.Examples(mnist_m_images,inputs=[input_image_classify], label = \"Select an example MNIST-M Image\") #working\n",
        "\n",
        "    with gr.Row():\n",
        "      gr.Markdown(value = f'MNIST- M Ground Truth Label = {[label for label in mnist_m_labels]}')\n",
        "\n",
        "\n",
        "    button_classify.click(fn=classify_image,\n",
        "                          inputs=[input_image_classify,radio_model],\n",
        "                          outputs=[label_classify])\n",
        "\n",
        "  with gr.Tab(\"Case 2 - Show both: MNIST_M_Both_Misclassify\"):\n",
        "\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        input_image_classify_both = gr.Image(label=\"Classify Digit\", type = \"pil\", height = 256, width = 256)\n",
        "        button_classify_both = gr.Button(\"Submit to Classify Image with Both Models\", visible = True, size ='sm')\n",
        "\n",
        "      with gr.Column():\n",
        "        with gr.Row():\n",
        "          label_classify_non_dann = gr.Label(label = \"NON DANN Predicted label\", num_top_classes=3, visible = True)\n",
        "        with gr.Row():\n",
        "          label_classify_dann = gr.Label(label = \"DANN Predicted label\", num_top_classes=3, visible = True)\n",
        "\n",
        "    mnist_m_images,mnist_m_labels = get_images_2()\n",
        "\n",
        "    with gr.Row():\n",
        "      gr.Examples(mnist_m_images,inputs=[input_image_classify_both], label = \"Select an example MNIST-M Image\") #working\n",
        "\n",
        "    with gr.Row():\n",
        "      gr.Markdown(value = f'MNIST- M Ground Truth Label = {[label for label in mnist_m_labels]}')\n",
        "\n",
        "\n",
        "    button_classify_both.click(fn=classify_image_both,\n",
        "                          inputs=[input_image_classify_both],\n",
        "                          outputs=[label_classify_non_dann,label_classify_dann])\n",
        "\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "EoglGKs0Skaw",
        "outputId": "9e8a2f13-8f99-4972-d78a-c1c68ec47b85"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://e80ef38a8495e62e3f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e80ef38a8495e62e3f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://e80ef38a8495e62e3f.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}